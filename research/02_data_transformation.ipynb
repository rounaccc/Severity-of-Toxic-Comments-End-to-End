{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Rounak\\\\Desktop\\\\OneDrive\\\\College\\\\Projects\\\\Severity-of-Toxic-Commentis-End-to-End'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    test_data_path: Path\n",
    "    train_data_path: Path\n",
    "    tokenizer_path: Path\n",
    "    max_features: int\n",
    "    maxpadlen: int\n",
    "    # potential_stopwords: list\n",
    "    # re_patterns: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SeverityOfToxicCommentsEndToEnd.utils.common import read_yaml, create_directories\n",
    "from SeverityOfToxicCommentsEndToEnd.constants import *\n",
    "from DataTransformationParam import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath = CONFIG_FILE_PATH, params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            train_data_path = config.train_data_path,\n",
    "            test_data_path = config.test_data_path,\n",
    "            tokenizer_path = config.tokenizer_path,\n",
    "            max_features = config.max_features,\n",
    "            maxpadlen = config.maxpadlen\n",
    "            # potential_stopwords = config.potential_stopwords,\n",
    "            # re_patterns = config.RE_PATTERNS\n",
    "        )\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rounak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from SeverityOfToxicCommentsEndToEnd.logging import logger\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import itertools\n",
    "from string import ascii_lowercase\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['editor', 'reference', 'thank', 'work', 'find', 'good', 'know', 'like', 'look', 'thing', 'want', 'time', 'list', 'section', 'wikipedia', 'doe', 'add', 'new', 'try', 'think', 'write', 'use', 'user', 'way', 'page']\n"
     ]
    }
   ],
   "source": [
    "print(potential_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.stopword_list = []\n",
    "        self.dual_alpha_list = []\n",
    "        self.train_text = []\n",
    "        self.lemma_train_text = []\n",
    "        self.processed_train_text = []\n",
    "    \n",
    "    def clean_text(self, text, remove_repeat_text=True, remove_patterns_text=True, is_lower=True):\n",
    "\n",
    "        if is_lower:\n",
    "            text=text.lower()\n",
    "            \n",
    "        if remove_patterns_text:\n",
    "            for target, patterns in RE_PATTERNS.items():\n",
    "                for pat in patterns:\n",
    "                    text=str(text).replace(pat, target)\n",
    "\n",
    "        if remove_repeat_text:\n",
    "            text = re.sub(r'(.)\\1{2,}', r'\\1', text) \n",
    "\n",
    "        text = str(text).replace(\"\\n\", \" \")\n",
    "        text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "        text = re.sub('[0-9]',\"\",text)\n",
    "        text = re.sub(\" +\", \" \", text)\n",
    "        text = re.sub(\"([^\\x00-\\x7F])+\",\" \",text)\n",
    "        return text \n",
    "    \n",
    "    def lemma(self, text, lemmatization=True):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        output=''\n",
    "        if lemmatization:\n",
    "            text=text.split(' ')\n",
    "            for word in text:\n",
    "                word1 = lemmatizer.lemmatize(word, pos = \"n\") #noun \n",
    "                word2 = lemmatizer.lemmatize(word1, pos = \"v\") #verb\n",
    "                word3 = lemmatizer.lemmatize(word2, pos = \"a\") #adjective\n",
    "                word4 = lemmatizer.lemmatize(word3, pos = \"r\") #adverb\n",
    "                output=output + \" \" + word4\n",
    "        else:\n",
    "            output=text\n",
    "        \n",
    "        return str(output.strip())\n",
    "    \n",
    "    def iter_all_strings(self):\n",
    "        for size in itertools.count(1):\n",
    "            for s in itertools.product(ascii_lowercase, repeat=size):\n",
    "                yield \"\".join(s)\n",
    "    \n",
    "    def dual_alpha(self):\n",
    "        for s in self.iter_all_strings():\n",
    "            self.dual_alpha_list.append(s)\n",
    "            if s == 'zz':\n",
    "                break\n",
    "    \n",
    "    def alter_dual_alpha(self):\n",
    "        self.dual_alpha_list.remove('i')\n",
    "        self.dual_alpha_list.remove('a')\n",
    "        self.dual_alpha_list.remove('am')\n",
    "        self.dual_alpha_list.remove('an')\n",
    "        self.dual_alpha_list.remove('as')\n",
    "        self.dual_alpha_list.remove('at')\n",
    "        self.dual_alpha_list.remove('be')\n",
    "        self.dual_alpha_list.remove('by')\n",
    "        self.dual_alpha_list.remove('do')\n",
    "        self.dual_alpha_list.remove('go')\n",
    "        self.dual_alpha_list.remove('he')\n",
    "        self.dual_alpha_list.remove('hi')\n",
    "        self.dual_alpha_list.remove('if')\n",
    "        self.dual_alpha_list.remove('is')\n",
    "        self.dual_alpha_list.remove('in')\n",
    "        self.dual_alpha_list.remove('me')\n",
    "        self.dual_alpha_list.remove('my')\n",
    "        self.dual_alpha_list.remove('no')\n",
    "        self.dual_alpha_list.remove('of')\n",
    "        self.dual_alpha_list.remove('on')\n",
    "        self.dual_alpha_list.remove('or')\n",
    "        self.dual_alpha_list.remove('ok')\n",
    "        self.dual_alpha_list.remove('so')\n",
    "        self.dual_alpha_list.remove('to')\n",
    "        self.dual_alpha_list.remove('up')\n",
    "        self.dual_alpha_list.remove('us')\n",
    "        self.dual_alpha_list.remove('we')\n",
    "\n",
    "        for letter in self.dual_alpha_list:\n",
    "            self.stopword_list.append(letter)\n",
    "    \n",
    "    def alter_stopwords(self):\n",
    "        for word in potential_stopwords:\n",
    "            self.stopword_list.append(word)\n",
    "        print(len(self.stopword_list))\n",
    "\n",
    "    def remove_stopwords(self, text, remove_stop=True):\n",
    "        output = \"\"\n",
    "        if remove_stop:\n",
    "            text=text.split(\" \")\n",
    "            for word in text:\n",
    "                if word not in self.stopword_list:\n",
    "                    output=output + \" \" + word\n",
    "        else :\n",
    "            output=text\n",
    "\n",
    "        return str(output.strip())\n",
    "    \n",
    "    def perform_data_transformation(self, path):\n",
    "        df = pd.read_csv(path)\n",
    "        for line in df['comment_text']: \n",
    "            self.train_text.append(self.clean_text(line))\n",
    "\n",
    "        for line in self.train_text:\n",
    "            self.lemma_train_text.append(self.lemma(line))\n",
    "        \n",
    "        self.dual_alpha()\n",
    "        self.alter_dual_alpha()\n",
    "        self.alter_stopwords()\n",
    "        for line in self.lemma_train_text: \n",
    "            self.processed_train_text.append(self.remove_stopwords(line))\n",
    "        \n",
    "        # convert to csv and store\n",
    "        df['comment_text'] = self.processed_train_text\n",
    "        df.to_csv(f'{self.config.root_dir}/processed_{path.split(\"/\")[-1]}', index=False)\n",
    "    \n",
    "    def tokenizer(self):\n",
    "        tokenizer = Tokenizer(num_words=self.config.max_features)\n",
    "        tokenizer.fit_on_texts(list(self.processed_train_text))\n",
    "        list_tokenized_train = tokenizer.texts_to_sequences(self.processed_train_text)\n",
    "        word_index=tokenizer.word_index\n",
    "        training_padded=pad_sequences(list_tokenized_train, maxlen=self.config.maxpadlen, padding = 'post')\n",
    "        # save tokenizer\n",
    "        with open(self.config.tokenizer_path, 'wb') as handle:\n",
    "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        # reset variables\n",
    "        self.stopword_list, self.dual_alpha_list, self.train_text, self.lemma_train_text, self.processed_train_text = [], [], [], [], []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-12 19:59:41,038: INFO: common] Successfully read yaml file from config\\config.yaml\n",
      "[2023-07-12 19:59:41,043: INFO: common] Successfully read yaml file from params.yaml\n",
      "[2023-07-12 19:59:41,046: INFO: common] Created directory at: artifacts\n",
      "[2023-07-12 19:59:41,049: INFO: common] Created directory at: artifacts/data_transformation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n",
      "700\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config = data_transformation_config)\n",
    "    data_transformation.perform_data_transformation(data_transformation_config.train_data_path)\n",
    "    data_transformation.tokenizer()\n",
    "    data_transformation.perform_data_transformation(data_transformation_config.test_data_path)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_data.csv'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_transformation_config.test_data_path.split(\"/\")[-1]\n",
    "# get only the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
