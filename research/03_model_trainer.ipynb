{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Rounak\\\\Desktop\\\\OneDrive\\\\College\\\\Projects\\\\Severity-of-Toxic-Commentis-End-to-End'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    processed_test_data_dir: Path\n",
    "    processed_train_data_dir: Path\n",
    "    tokenizer_dir: Path\n",
    "    epochs: int\n",
    "    embedding_dim: int\n",
    "    batch_size: int\n",
    "    fasttext_model_path: Path\n",
    "    max_features: int\n",
    "    maxpadlen: int\n",
    "    tokenizer_dir: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SeverityOfToxicCommentsEndToEnd.utils.common import read_yaml, create_directories\n",
    "from SeverityOfToxicCommentsEndToEnd.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath = CONFIG_FILE_PATH, params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        params = self.params.TransformationAndTrainingArguments\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            processed_test_data_dir = config.processed_test_data_dir,\n",
    "            processed_train_data_dir = config.processed_train_data_dir,\n",
    "            tokenizer_dir = config.tokenizer_dir,\n",
    "            epochs = params.epochs,\n",
    "            embedding_dim = params.embedding_dim,\n",
    "            batch_size = params.batch_size,\n",
    "            fasttext_model_path = params.fasttext_model_path,\n",
    "            max_features = params.max_features,\n",
    "            maxpadlen = params.maxpadlen\n",
    "        )\n",
    "        return model_trainer_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "        self.embeddings_index_fasttext = {}\n",
    "        self.word_index = None\n",
    "        self.embedding_matrix_fasttext = None\n",
    "        self.train_padded = []\n",
    "        self.test_padded = []\n",
    "        self.train_labels = None\n",
    "        self.test_labels = None\n",
    "        self.df = None\n",
    "        self.df_test = None\n",
    "    \n",
    "    def DefineTokenizer(self, path):\n",
    "        self.df = pd.read_csv(path)\n",
    "        self.df = self.df[self.df['comment_text'].notnull()]\n",
    "        processed_train_text = self.df['comment_text'].to_list()\n",
    "        self.df_test = pd.read_csv(self.config.processed_test_data_dir)\n",
    "        self.df_test = self.df_test[self.df_test['comment_text'].notnull()]\n",
    "        processed_test_text = self.df_test['comment_text'].to_list()\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            tokenizer = Tokenizer(num_words=self.config.max_features)\n",
    "            tokenizer.fit_on_texts(list(processed_train_text))\n",
    "            with open(self.config.tokenizer_dir, 'wb') as handle:\n",
    "                pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        else:\n",
    "            tokenizer = Tokenizer(num_words=self.config.max_features)\n",
    "            tokenizer.fit_on_texts(list(processed_train_text))\n",
    "            list_tokenized_train = tokenizer.texts_to_sequences(processed_train_text)\n",
    "            self.word_index = tokenizer.word_index\n",
    "            self.train_padded = pad_sequences(list_tokenized_train, maxlen=self.config.maxpadlen, padding='post')\n",
    "            list_tokenized_test = tokenizer.texts_to_sequences(processed_test_text)\n",
    "            self.test_padded = pad_sequences(list_tokenized_test, maxlen=self.config.maxpadlen, padding='post')\n",
    "\n",
    "    def InitializeFastText(self):\n",
    "        f = open(self.config.fasttext_model_path, encoding='utf8')\n",
    "        for line in f:\n",
    "            line.encode('utf-8').strip()\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            self.embeddings_index_fasttext[word] = np.asarray(values[1:], dtype='float32')\n",
    "        f.close()\n",
    "        self.embedding_matrix_fasttext = np.random.random((len(self.word_index) + 1, self.config.embedding_dim))\n",
    "        for word, i in self.word_index.items():\n",
    "            embedding_vector = self.embeddings_index_fasttext.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                self.embedding_matrix_fasttext[i] = embedding_vector\n",
    "            \n",
    "    def LSTMTrainer(self):\n",
    "        self.train_labels = np.array(self.df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']])\n",
    "        self.test_labels = np.array(self.df_test[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']])\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(len(self.word_index) + 1,\n",
    "                                self.config.embedding_dim,\n",
    "                                weights = [self.embedding_matrix_fasttext],\n",
    "                                input_length = self.config.maxpadlen,\n",
    "                                trainable=False,\n",
    "                                name = 'embeddings'),\n",
    "        tf.keras.layers.Input(shape=(self.config.maxpadlen, ),dtype='int32'),\n",
    "        tf.keras.layers.LSTM(40,return_sequences=True, name='lstm_layer'),\n",
    "        tf.keras.layers.GlobalMaxPooling1D(),\n",
    "        tf.keras.layers.Dropout(.1),\n",
    "        tf.keras.layers.Dense(30, activation='relu', kernel_initializer='he_uniform'),\n",
    "            tf.keras.layers.Dropout(.1),\n",
    "            tf.keras.layers.Dense(6, activation='sigmoid', kernel_initializer='glorot_uniform')\n",
    "        ])\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "\n",
    "        history = model.fit(self.train_padded, self.train_labels, epochs=self.config.epochs, batch_size=self.config.batch_size, validation_data=(self.test_padded, self.test_labels))\n",
    "        model.save(self.config.root_dir,'/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-15 13:17:40,168: INFO: common] Successfully read yaml file from config\\config.yaml\n",
      "[2023-07-15 13:17:40,181: INFO: common] Successfully read yaml file from params.yaml\n",
      "[2023-07-15 13:17:40,186: INFO: common] Created directory at: artifacts\n",
      "[2023-07-15 13:17:40,189: INFO: common] Created directory at: artifacts/model_trainer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embeddings (Embedding)      (None, 200, 300)          38906100  \n",
      "                                                                 \n",
      " input_3 (InputLayer)        multiple                  0         \n",
      "                                                                 \n",
      " lstm_layer (LSTM)           (None, 200, 40)           54560     \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Gl  (None, 40)                0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 40)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 30)                1230      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 30)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 6)                 186       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 38962076 (148.63 MB)\n",
      "Trainable params: 55976 (218.66 KB)\n",
      "Non-trainable params: 38906100 (148.41 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "3989/3989 [==============================] - 401s 98ms/step - loss: 0.0673 - accuracy: 0.8997 - val_loss: 0.0504 - val_accuracy: 0.9941\n",
      "Epoch 2/2\n",
      "3989/3989 [==============================] - 399s 100ms/step - loss: 0.0500 - accuracy: 0.9863 - val_loss: 0.0466 - val_accuracy: 0.9941\n",
      "[2023-07-15 13:33:07,138: INFO: builder_impl] Assets written to: artifacts/model_trainer\\assets\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    model_trainer = ModelTrainer(config = model_trainer_config)\n",
    "    model_trainer.DefineTokenizer(model_trainer_config.processed_train_data_dir)\n",
    "    model_trainer.InitializeFastText()\n",
    "    model_trainer.LSTMTrainer()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
